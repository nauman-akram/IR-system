{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ewMBMq3-VJDd"
   },
   "source": [
    "<h2><center>Building an Information Retrieval System</center></h2>\n",
    "<h3><center>Sparse matrix implementation</center></h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DbVFxmnOVJDj"
   },
   "outputs": [],
   "source": [
    "#Improt the required Libraries\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NciHjAQHVJDv"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "VAEoL6X7V03X",
    "outputId": "6437e26f-6bfd-4e9f-baf0-6591d13043c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "UuvO7Fg8WLgA",
    "outputId": "3a19eee8-23ef-4f3a-8844-ea85e21504df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gdrive/My Drive/Classroom/Tools and Techniques for Data Science MS\n"
     ]
    }
   ],
   "source": [
    "%cd /gdrive/My Drive/Classroom/Tools and Techniques for Data Science MS/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EMLN_0GNXDwn"
   },
   "outputs": [],
   "source": [
    "#!unzip 'Assignment 1.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "INsqy_VEXNAu",
    "outputId": "c913ac95-86a3-4355-9242-6346581563a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gdrive/My Drive/Classroom/Tools and Techniques for Data Science MS/Assignment 1\n"
     ]
    }
   ],
   "source": [
    "%cd Assignment\\ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tXZtcZ3fXl4-"
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Mk2VezktVJDx",
    "outputId": "c77864a8-1a47-4fa1-ee05-302300079e42"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reading the directory of our dataset of documents\n",
    "files = os.listdir('data/')\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 495
    },
    "colab_type": "code",
    "id": "-DsDuDUsVJD6",
    "outputId": "14a7dace-4080-4356-82dd-bf8a95972a44",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 files have been processed\n",
      "1000 files have been processed\n",
      "1500 files have been processed\n",
      "2000 files have been processed\n",
      "2500 files have been processed\n",
      "3000 files have been processed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Proceedings',\n",
       " 'of',\n",
       " 'the',\n",
       " '12th',\n",
       " 'Conference',\n",
       " 'of',\n",
       " 'the',\n",
       " 'European',\n",
       " 'Chapter',\n",
       " 'of',\n",
       " 'the',\n",
       " 'ACL,',\n",
       " 'pages',\n",
       " '719\\x96727,',\n",
       " 'Athens,',\n",
       " 'Greece,',\n",
       " '30',\n",
       " 'March',\n",
       " '\\x96',\n",
       " '3']"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The following code takes all files, replaces new line character with space character and put words of all files in the Words list \n",
    "Words = []\n",
    "x = 0\n",
    "for f in files:\n",
    "    fhand = open(\"data/\"+f,encoding='Latin-1')\n",
    "    file = fhand.read()\n",
    "    file = file.replace(\"\\n\",\" \")\n",
    "    for word in file.split(\" \"):\n",
    "        Words.append(word)\n",
    "    x += 1\n",
    "    \n",
    "    if x%500 == 0:\n",
    "        print(x, \"files have been processed\")\n",
    "\n",
    "Words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "fr4Dzq7yVJEB",
    "outputId": "a1e51549-e7d0-4e53-db7a-78ae78e10f28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Proceedings', 'of', 'the', '12th', 'Conference'] [24830, 498335, 809157, 357, 12477]\n"
     ]
    }
   ],
   "source": [
    "#find count of unique words in all these files and store it in a dictionary\n",
    "Word_To_Count = {}\n",
    "from collections import Counter\n",
    "\n",
    "Word_To_Count = dict(Counter(Words))\n",
    "\n",
    "\n",
    "lgCountK = list(Word_To_Count.keys())[:5]\n",
    "lgCountV = list(Word_To_Count.values())[:5]\n",
    "\n",
    "print(lgCountK,lgCountV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "jwrb2JjdVJEH",
    "outputId": "49da6f3e-f9ca-4d84-d601-dc138c17a2c0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print the count of Sweden\n",
    "Word_To_Count[\"Sweden\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fsnGGTiQVJEO"
   },
   "outputs": [],
   "source": [
    "#convert all text into lower case then find count of unique words,\n",
    "Small_Words = []\n",
    "#read each file, split all words and append the lower case word in Small_Words list\n",
    "x= 0\n",
    "\n",
    "for f in files:\n",
    "    fhand = open(\"data/\"+f, encoding='Latin-1')\n",
    "    file = fhand.read()\n",
    "    file = file.replace(\"\\n\",\" \")\n",
    "    file = file.lower()\n",
    "    for word in file.split(\" \"):\n",
    "        Small_Words.append(word)\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "5q0q-HoeVJEU",
    "outputId": "c4ed209c-ce45-493d-b625-9dfe67f0b105"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['proceedings', 'of', 'the', '12th', 'conference', 'of', 'the', 'european', 'chapter', 'of', 'the', 'acl,', 'pages', '719\\x96727,', 'athens,', 'greece,', '30', 'march', '\\x96', '3']\n"
     ]
    }
   ],
   "source": [
    "print(Small_Words[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qMyR_opaVJEb"
   },
   "outputs": [],
   "source": [
    "#find count of unique words after converting them to lowercase in all these files and store it in a dictionary\n",
    "Small_Word_To_Count = {}\n",
    "Small_Word_To_Count = dict(Counter(Small_Words))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "qr8aiyqKVJEh",
    "outputId": "ca9169ac-b3c6-4915-e990-3993fce94076"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['proceedings', 'of', 'the', '12th', 'conference'] [25097, 499737, 909196, 357, 14013]\n"
     ]
    }
   ],
   "source": [
    "smCountK = list(Small_Word_To_Count.keys())[:5]\n",
    "smCountV = list(Small_Word_To_Count.values())[:5]\n",
    "\n",
    "print(smCountK,smCountV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "eNFhqkKnfYxu",
    "outputId": "fdd325d8-34c4-46a3-f78e-5b940c107be7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2776"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print the count of Sweden\n",
    "Small_Word_To_Count[\"nlp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5kM8VKem0H2q"
   },
   "outputs": [],
   "source": [
    "def getkey(item):\n",
    "    return item[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "y7aPpmqFVJEm",
    "outputId": "7311ab6c-5a1e-43a2-d9d7-50a63bbb141b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 909196), ('', 830096), ('of', 499737), ('and', 394978), ('in', 335293), ('a', 311651), ('to', 278858), ('is', 207969), ('for', 205435), ('.', 167638), ('we', 144382), ('that', 130465), ('as', 107944), ('are', 104371), ('on', 104296), ('with', 96599), ('this', 84446), ('by', 82984), ('be', 77896), ('from', 66493)]\n"
     ]
    }
   ],
   "source": [
    "#sort the small words (in Small_Word_To_Count dictionary) by frequencies and store the results in freq.txt file\n",
    "#Each line of the file should have word seperated by comma with its counts\n",
    "#Hint: Can you use tuples to sort key value pairs in a list?\n",
    "\n",
    "to_b_stored_sm = tuple(Small_Word_To_Count.items())\n",
    "to_b_stored_sm= sorted(to_b_stored_sm, key=getkey, reverse=True)\n",
    "smCountK = to_b_stored_sm[:20]\n",
    "\n",
    "print(smCountK)\n",
    "\n",
    "fhand = open(\"freq.txt\",mode='w')\n",
    "for x in to_b_stored_sm:\n",
    "    fhand.write(list(x)[0])\n",
    "    fhand.write(',')\n",
    "    fhand.write(str(list(x)[1]))\n",
    "    fhand.write('\\n')\n",
    "    \n",
    "fhand.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "bveGfApwVJEu",
    "outputId": "ee789a91-4810-4521-a23b-bf3e10a22143"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '\\x00', '\\x00\\x00', '\\x00\\x00\\x00', '\\x00\\x00\\x00\\x00', '\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00', '\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01', '\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01', '\\x00\\x00\\x01\\x01', '\\x00\\x00\\t', '\\x00\\x00!', '\\x00\\x01', '\\x00\\x01\\x00\\x01\\x00', '\\x00\\x01\\x00\\x02\\x00', '\\x00\\x02\\x01', '\\x00\\x02\\x01\\x04\\x03', '\\x00\\x02\\x01\\x04\\x03\\x06\\x05\\x08\\x07', '\\x00\\x07\\x06', '\\x00\\x0e\\x13\\x15\\x14', '\\x00\\x19\\x18ff\\x1a']\n"
     ]
    }
   ],
   "source": [
    "#sort the words by alphabatical order and store results in words.txt file\n",
    "#Each line of the file should have word seperated by comma with its counts\n",
    "fhand = open(\"words.txt\",mode='w')\n",
    "\n",
    "Word_To_Count_lg = sorted(Word_To_Count.keys())\n",
    "print(Word_To_Count_lg[:20])\n",
    "\n",
    "\n",
    "for x in Word_To_Count_lg:\n",
    "    fhand.write(x)\n",
    "    fhand.write(',')\n",
    "    fhand.write(str(Word_To_Count[x]))\n",
    "    fhand.write('\\n')\n",
    "    \n",
    "fhand.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "colab_type": "code",
    "id": "6oTLL-KfVJEz",
    "outputId": "6b9a3929-2f59-4a1f-8161-944c6056de30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of vocab:  694084 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\"--t',\n",
       " '\"--~',\n",
       " '\"-.',\n",
       " '\"-...',\n",
       " '\"-4',\n",
       " '\"-:_u',\n",
       " '\"->',\n",
       " '\"->)(to',\n",
       " '\"-\\\\]--',\n",
       " '\"-able\"',\n",
       " '\"-c\"',\n",
       " '\"-drink',\n",
       " '\"-e4~\"',\n",
       " '\"-ed\"',\n",
       " '\"-ed\").',\n",
       " '\"-fe/-ve\"',\n",
       " '\"-fn->\").',\n",
       " '\"-ga',\n",
       " '\"-he',\n",
       " '\"-human)']"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort the list of Small_Words alphabatically and store it in a new list called Vocabulary\n",
    "# Hint: Explore Set Function\n",
    "\n",
    "Vocabulary = set(Small_Word_To_Count) #it will remove the duplicate values but because of set property you can not say what order it might have\n",
    "Vocabulary = sorted(Vocabulary)\n",
    "print(\"len of vocab: \", len(Vocabulary),\"\\n\")\n",
    "Vocabulary[3000:3020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UttVL42DVJE5"
   },
   "outputs": [],
   "source": [
    "# Now that the list is sorted alphabatically, \n",
    "# We can create dictionaries\n",
    "Word_to_ID = {}\n",
    "# Go through the overview.ipynb notebook and convert list items into tuples of indexes and value\n",
    "# putt all these tuples into Word_to_ID dictionary, Now we can search every words ID\n",
    "\n",
    "tuples_of_vocab = enumerate(Vocabulary)\n",
    "temp_dixt = dict(tuples_of_vocab)\n",
    "\n",
    "Word_to_ID = dict(zip(temp_dixt.values(), temp_dixt.keys()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "AH9U5kMgkFgv",
    "outputId": "a42b30a9-6045-4ab0-e995-c68f970a00db"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "527055"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Word_to_ID['proceedings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pB_KEFbwkhFM"
   },
   "outputs": [],
   "source": [
    "#temp = {1:\"abc\", 3:\"xya\", 2:\"nbc\"}\n",
    "#{(v,k) for k,v in temp.items()}\n",
    "#dict(zip(temp.values(), temp.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nHVj6xx7VJE-"
   },
   "outputs": [],
   "source": [
    "# Now inverse the key value pairs of Word_to_ID Dictionary, so that we can search each word by its ID\n",
    "ID_to_Word = {}\n",
    "ID_to_Word = dict(zip(Word_to_ID.values(), Word_to_ID.keys()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Rkp6Blsnlg5k",
    "outputId": "79c361e7-e467-47d0-f14e-767a9f511f18"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"amount\"'"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ID_to_Word[3809]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mcw1uoHqVJFC"
   },
   "source": [
    "# An introduction to Sparse Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RWlrZfNKVJFE"
   },
   "source": [
    "A sparse matrix is a matrix that is comprised of mostly zero values.\n",
    "Sparse matrices are distinct from matrices with mostly non-zero values, which are referred to as dense matrices.\n",
    "\n",
    "\"A matrix is sparse if many of its coefficients are zero. The interest in sparsity arises because its exploitation can lead to enormous computational savings and because many large matrix problems that occur in practice are sparse.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZjfK582cVJFG"
   },
   "source": [
    "## Problems with Sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zqPAdZ0eVJFI"
   },
   "source": [
    "### Space Complexity\n",
    "Very large matrices require a lot of memory, and some very large matrices that we wish to work with are sparse.\n",
    "An example of a very large matrix that is too large to be stored in memory is a link matrix that shows the links from one website to another.\n",
    "\n",
    "In this case, the matrix contained is sparse with many more zero values than data values. The problem with representing these sparse matrices as dense matrices is that memory is required and must be allocated for each 32-bit or even 64-bit zero value in the matrix.\n",
    "\n",
    "This is clearly a waste of memory resources as those zero values do not contain any information.\n",
    "\n",
    "### Time Complexity\n",
    "Assuming a very large sparse matrix can be fit into memory, we will want to perform operations on this matrix.\n",
    "\n",
    "Simply, if the matrix contains mostly zero-values, i.e. no data, then performing operations across this matrix may take a long time where the bulk of the computation performed will involve adding or multiplying zero values together.\n",
    "\n",
    "##### Learn More About Sparse Matrrices on Internet. You have to work on sparse matrices in the following tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U15JYELpVJFM"
   },
   "source": [
    "#### Read each document and save it in a Term Document matrix\n",
    "#### The Size of document vector should be equal to the size of vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X8pucZFqVJFN"
   },
   "outputs": [],
   "source": [
    "#Your Code Here\n",
    "import numpy as np\n",
    "from scipy.sparse import dok_matrix\n",
    "\n",
    "\n",
    "tdMatrix = dok_matrix((len(files), len(Vocabulary)), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1J2IHoPkVJFT"
   },
   "source": [
    "### Create Term Document Matrix Having Frequency of each word (Sparse Matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PyW54MMVVJFV"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "for id, f in enumerate(files):\n",
    "    fhand = open(\"data/\"+f, encoding='Latin-1')\n",
    "    file = fhand.read()\n",
    "    file = file.replace(\"\\n\",\" \")\n",
    "    file = file.lower()\n",
    "    file = file.split(\" \")\n",
    "    for word in file:\n",
    "        tdMatrix[id,Word_to_ID[word]] = file.count(word)\n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V5Ao5S04NN3y"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "data = {\"vocab\":Vocabulary, \"files\":files, \"Word_to_ID\":Word_to_ID, \"tdMatrix\": tdMatrix}\n",
    "with open('data.pkl', 'wb') as f:\n",
    "    pickle.dump(data,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yxoD9RkaVJFb"
   },
   "outputs": [],
   "source": [
    "def SearchForReleventDoc(Query, query_vector,Vocabulary,Word_to_ID, tdMatrix):    \n",
    "    #Take Query from the User \n",
    "    #Convert the query into the query vector, This will be same as we converted each document into document vector\n",
    "    Query = Query.lower()\n",
    "    Query_vector = Query.split(' ')\n",
    "    #Query vector should have frequency of words\n",
    "    \n",
    "    for word in Query_vector:\n",
    "        if word in Vocabulary:\n",
    "            #print(\"word : \",Query_vector.count(word))\n",
    "            query_vector[0,Word_to_ID[word]] = Query_vector.count(word)\n",
    "\n",
    "    #query_vector.shape\n",
    "    #query_vector.nonzero()\n",
    "    #Take the dot product of query vector with each document vector and rank them, Avoid for loops\n",
    "    results = tdMatrix.dot(query_vector.transpose())\n",
    "    results = results.toarray()\n",
    "    results  = results.flatten()\n",
    "    print(results.argmax())\n",
    "    idx = (-results).argsort()\n",
    "    #idx = reversed(idx)\n",
    "    top_idx = idx[:5]\n",
    "    top_score = results[top_idx]\n",
    "    return top_idx, top_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zazicC2GVJFg"
   },
   "outputs": [],
   "source": [
    "#Print the file names\n",
    "def print_files(files, top_idx, top_score):\n",
    "    print(\"top 5 files....\")\n",
    "    for i,file_index in enumerate(top_idx):\n",
    "        print(files[i],\"-> score:\",top_score[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164
    },
    "colab_type": "code",
    "id": "uEuQvbbWXfhR",
    "outputId": "b0fe086b-d62b-4e13-8201-5301a2236371"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the search query: Natural language Processing\n",
      "1972\n",
      "top 5 files....\n",
      "E09-1082.pdf.txt -> score: 172\n",
      "E09-2010.pdf.txt -> score: 134\n",
      "E09-2016.pdf.txt -> score: 124\n",
      "E09-2002.pdf.txt -> score: 118\n",
      "E09-1090.pdf.txt -> score: 116\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.sparse import dok_matrix\n",
    "\n",
    "with open('data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "    \n",
    "Vocabulary = data[\"vocab\"]\n",
    "Word_to_ID = data[\"Word_to_ID\"]\n",
    "loaded_tdMatrix = data['tdMatrix']\n",
    "files = data['files']\n",
    "\n",
    "query_vector = dok_matrix((1, len(Vocabulary)), dtype=np.int32)   \n",
    "Query = input(\"Enter the search query: \")\n",
    "top_idx, top_score = SearchForReleventDoc(Query,query_vector,Vocabulary,Word_to_ID, loaded_tdMatrix)\n",
    "\n",
    "print_files(files, top_idx, top_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m-mjhrgglrHi"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "19041_A1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
